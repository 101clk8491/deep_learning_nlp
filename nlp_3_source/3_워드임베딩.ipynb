{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 임베딩(Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 영어/한국어 Word2Vec 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 챕터에서는 영어와 한국어 훈련 데이터에 대해서 Word2Vec을 학습해보겠습니다. gensim 패키지에서 Word2Vec은 이미 구현되어져 있으므로, 별도로 Word2Vec을 구현할 필요없이 손쉽게 훈련시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 영어 Word2Vec 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 영어 데이터를 다운로드 받아 직접 Word2Vec 작업을 진행해보겠습니다. 파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 훈련 데이터 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크 : https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\n",
    "\n",
    "위 링크에서 훈련 데이터를 다운로드 받을 수 있습니다. 위 zip 파일의 압축을 풀면 ted_en-20160408.xml이라는 파일이 있는데, 이를 훈련 데이터로 사용할 예정입니다.\n",
    "\n",
    "아래는 해당 xml 파일의 형식을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이터 로드 및 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 다운로드하고, xml 파일로부터 필요한 내용만을 가져와 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "# 데이터 다운로드\n",
    "\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "  target_text = etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "  parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로드한 데이터 중에서 300개의 글자(characters)만 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "\n",
    "sent_text=sent_tokenize(content_text)\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 수행 후의 총 샘플의 개수는 몇 개일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273424\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 샘플의 개수는 약 27만 3천개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]: # 샘플 3개만 출력\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 3개 문장만 출력해보았는데 토큰화가 수행되었음을 볼 수 있습니다. 이제 Word2Vec 모델에 텍스트 데이터를 훈련시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from gensim) (1.18.1)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.34-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.18.0,>=1.17.34\n",
      "  Downloading botocore-1.17.34-py2.py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from botocore<1.18.0,>=1.17.34->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110323 sha256=4f972bdddfb8f8172f801dd697e342463f18f6d2173ddb9ae00e8382a394b872\n",
      "  Stored in directory: c:\\users\\jikim\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, smart-open, Cython, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "Successfully installed Cython-0.29.14 boto3-1.14.34 botocore-1.17.34 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Word2Vec의 하이퍼파라미터값은 다음과 같습니다.\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "window = 컨텍스트 윈도우 크기\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "workers = 학습을 위한 프로세스 수\n",
    "sg = 0은 CBOW, 1은 Skip-gram.\n",
    "\n",
    "이제 Word2Vec에 대해서 학습을 진행하였습니다. Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원합니다. man과 가장 유사한 단어들은 어떤 단어들일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8430303335189819), ('guy', 0.7995673418045044), ('lady', 0.7713043689727783), ('boy', 0.7505444288253784), ('gentleman', 0.7413794994354248), ('girl', 0.7359131574630737), ('soldier', 0.7012191414833069), ('kid', 0.6932430267333984), ('poet', 0.6773760318756104), ('philosopher', 0.6686331033706665)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있습니다. 이제 Word2Vec를 통해 단어의 유사도를 계산할 수 있게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Word2Vec 모델 저장하고 로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다. 이 모델을 가지고 시각화 챕터에서 시각화를 진행할 예정이므로 꼭 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로드한 모델에 대해서 다시 man과 유사한 단어를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8430303335189819), ('guy', 0.7995673418045044), ('lady', 0.7713043689727783), ('boy', 0.7505444288253784), ('gentleman', 0.7413794994354248), ('girl', 0.7359131574630737), ('soldier', 0.7012191414833069), ('kid', 0.6932430267333984), ('poet', 0.6773760318756104), ('philosopher', 0.6686331033706665)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x195c2070048>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# NULL 값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식을 통한 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5] # 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 72\n",
      "리뷰의 평균 길이 : 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('채민서', 0.8514694571495056), ('서영희', 0.8469486236572266), ('송강호', 0.844077467918396), ('안성기', 0.8389574289321899), ('김명민', 0.8383212089538574), ('윤제문', 0.8376558423042297), ('정재영', 0.8364806175231934), ('이주승', 0.8344547152519226), ('이정재', 0.8342359066009521), ('미스캐스팅', 0.8319401741027832)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('슬래셔', 0.8894140720367432), ('호러', 0.8726279735565186), ('무협', 0.8616793155670166), ('정통', 0.8468502759933472), ('느와르', 0.8397258520126343), ('물의', 0.8378305435180664), ('무비', 0.8368439674377441), ('물', 0.8256744742393494), ('블록버스터', 0.8207323551177979), ('멜로', 0.7997592687606812)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 한국어 Word2Vec 만들기(위키피디아)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 위키피디아 한국어 덤프 파일을 다운받아서 한국어로 Word2Vec을 직접 진행해보겠습니다. 영어와 크게 다른 점은 없지만 한국어는 형태소 토큰화를 해야만 좋은 성능을 얻을 수 있습니다. 간단히 말해 형태소 분석기를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 위키피디아 한국어 덤프 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dumps.wikimedia.org/kowiki/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 링크에는 많은 위키피디아 덤프 파일들이 존재합니다. 그 중에서 사용할 데이터는 kowiki-latest-pages-articles.xml.bz2 파일입니다. 해당 파일은 xml 파일므로, Word2Vec을 원활하게 진행하기 위해 파일 형식을 변환해줄 필요가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 위키피디아 익스트랙터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 파일을 모두 다운로드 받았다면 위키피디아 덤프 파일을 텍스트 형식으로 변환시켜주는 오픈소스인 '위키피디아 익스트랙터'를 사용할 것입니다. '위키피디아 익스트랙터'를 다운로드 받기 위해서는 윈도우의 명령 프롬프트나 MAC과 리눅스의 터미널에서 아래의 git clone 명령어를 통해 다운로드 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikiextractor in c:\\users\\jikim\\anaconda3\\lib\\site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 위키피디아 한국어 덤프 파일 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위키피디아 익스트랙터와 위키피디아 한국어 덤프 파일을 동일한 디렉토리 경로에 두고, 아래 명령어를 실행하면 위키피디아 덤프 파일이 텍스트 파일로 변환됩니다. 컴퓨터마다 다르지만 보통 10분 내외의 시간이 걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일로 변환된 위키피디아 한국어 덤프는 총 6개의 디렉토리(2018년 10월 기준)로 구성되어져 있습니다. AA ~ AF의 디렉토리로 각 디렉토리 내에는 wiki_00 ~ wiki_90이라는 파일들이 들어있습니다. 각 파일들을 열어보면 이와 같은 구성이 반복되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어서 AA 디렉토리의 wiki_00 파일을 읽어보면, 지미 카터에 대한 내용이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 6개 AA ~ AF 디렉토리 안의 wiki00 ~ wiki90 파일들을 하나의 텍스트 파일로 통합하겠습니다. (만약, 더 간단히 하고 싶다면 모든 디렉토리 파일을 통합하지 않고, 하나의 디렉토리 내의 파일들에 대해서만 통합 작업을 진행하고 모델의 입력으로 사용할수도있습니다. 하지만 모델의 성능은 전체 파일에 대해서 진행한 경우보다 좋지 않을 수 있습니다.)\n",
    "\n",
    "작업은 6개의 디렉토리 내 파일들에 대해서 각 하나의 파일로 통합 후, 6개의 파일을 다시 하나로 통합하는 순서로 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 훈련 데이터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 AA 디렉토리 안의 모든 파일인 wiki00 ~ wiki90에 대해서 wikiAA.txt로 통합해보겠습니다. 프롬프트에서 아래의 커맨드를 수행합니다. (윈도우 환경 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy AA디렉토리의 경로\\wiki* wikiAA.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 커맨드는 AA디렉토리 안의 wiki로 시작되는 모든 파일을 wikiAA.txt 파일에 전부 복사하라는 의미를 담고있습니다. 결과적으로 wiki00 ~ wiki90파일의 모든 내용은 wikiAA.txt 파일이라는 하나의 파일에 내용이 들어가게 됩니다.\n",
    "\n",
    "각 디렉토리에 대해서도 동일하게 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy AB디렉토리의 경로\\wiki* wikiAB.txt  \n",
    "copy AC디렉토리의 경로\\wiki* wikiAC.txt  \n",
    "copy AD디렉토리의 경로\\wiki* wikiAD.txt  \n",
    "copy AE디렉토리의 경로\\wiki* wikiAE.txt  \n",
    "copy AF디렉토리의 경로\\wiki* wikiAF.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 되면 현재 경로에는 각 디렉토리의 파일들을 하나로 합친 wikiAA.txt 부터 wikiAF.txt라는 6개의 파일이 생깁니다. 그럼 이제 이 파일들에 대해서도 하나의 파일로 합치는 작업을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy 현재 디렉토리의 경로\\wikiA* wiki_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 텍스트 파일을 하나로 만든 훈련 데이터가 완성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 훈련 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('wiki_data.txt', encoding=\"utf8\")\n",
    "# 예를 들어 위도우 바탕화면에서 작업한 저자의 경우\n",
    "# f = open(r'C:\\Users\\USER\\Desktop\\wiki_data.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 파일을 불러왔습니다. 파일이 정상적으로 저장되었는지 5개의 줄만 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 줄 :<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "\n",
      "2번째 줄 :지미 카터\n",
      "\n",
      "3번째 줄 :제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "4번째 줄 :지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "\n",
      "5번째 줄 :1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if line != '\\n':\n",
    "        i=i+1\n",
    "        print(\"%d번째 줄 :\"%i + line)\n",
    "    if i==5:\n",
    "        break \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 출력되는 것을 볼 수 있습니다. 이제 본격적으로 Word2Vec을 위한 학습 데이터를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000번째 While문.\n",
      "10000번째 While문.\n",
      "15000번째 While문.\n",
      "20000번째 While문.\n",
      "25000번째 While문.\n",
      "30000번째 While문.\n",
      "35000번째 While문.\n",
      "40000번째 While문.\n",
      "45000번째 While문.\n",
      "50000번째 While문.\n",
      "55000번째 While문.\n",
      "60000번째 While문.\n",
      "65000번째 While문.\n",
      "70000번째 While문.\n",
      "75000번째 While문.\n",
      "80000번째 While문.\n",
      "85000번째 While문.\n",
      "90000번째 While문.\n",
      "95000번째 While문.\n",
      "100000번째 While문.\n",
      "105000번째 While문.\n",
      "110000번째 While문.\n",
      "115000번째 While문.\n",
      "120000번째 While문.\n",
      "125000번째 While문.\n",
      "130000번째 While문.\n",
      "135000번째 While문.\n",
      "140000번째 While문.\n",
      "145000번째 While문.\n",
      "150000번째 While문.\n",
      "155000번째 While문.\n",
      "160000번째 While문.\n",
      "165000번째 While문.\n",
      "170000번째 While문.\n",
      "175000번째 While문.\n",
      "180000번째 While문.\n",
      "185000번째 While문.\n",
      "190000번째 While문.\n",
      "195000번째 While문.\n",
      "200000번째 While문.\n",
      "205000번째 While문.\n",
      "210000번째 While문.\n",
      "215000번째 While문.\n",
      "220000번째 While문.\n",
      "225000번째 While문.\n",
      "230000번째 While문.\n",
      "235000번째 While문.\n",
      "240000번째 While문.\n",
      "245000번째 While문.\n",
      "250000번째 While문.\n",
      "255000번째 While문.\n",
      "260000번째 While문.\n",
      "265000번째 While문.\n",
      "270000번째 While문.\n",
      "275000번째 While문.\n",
      "280000번째 While문.\n",
      "285000번째 While문.\n",
      "290000번째 While문.\n",
      "295000번째 While문.\n",
      "300000번째 While문.\n",
      "305000번째 While문.\n",
      "310000번째 While문.\n",
      "315000번째 While문.\n",
      "320000번째 While문.\n",
      "325000번째 While문.\n",
      "330000번째 While문.\n",
      "335000번째 While문.\n",
      "340000번째 While문.\n",
      "345000번째 While문.\n",
      "350000번째 While문.\n",
      "355000번째 While문.\n",
      "360000번째 While문.\n",
      "365000번째 While문.\n",
      "370000번째 While문.\n",
      "375000번째 While문.\n",
      "380000번째 While문.\n",
      "385000번째 While문.\n",
      "390000번째 While문.\n",
      "395000번째 While문.\n",
      "400000번째 While문.\n",
      "405000번째 While문.\n",
      "410000번째 While문.\n",
      "415000번째 While문.\n",
      "420000번째 While문.\n",
      "425000번째 While문.\n",
      "430000번째 While문.\n",
      "435000번째 While문.\n",
      "440000번째 While문.\n",
      "445000번째 While문.\n",
      "450000번째 While문.\n",
      "455000번째 While문.\n",
      "460000번째 While문.\n",
      "465000번째 While문.\n",
      "470000번째 While문.\n",
      "475000번째 While문.\n",
      "480000번째 While문.\n",
      "485000번째 While문.\n",
      "490000번째 While문.\n",
      "495000번째 While문.\n",
      "500000번째 While문.\n",
      "505000번째 While문.\n",
      "510000번째 While문.\n",
      "515000번째 While문.\n",
      "520000번째 While문.\n",
      "525000번째 While문.\n",
      "530000번째 While문.\n",
      "535000번째 While문.\n",
      "540000번째 While문.\n",
      "545000번째 While문.\n",
      "550000번째 While문.\n",
      "555000번째 While문.\n",
      "560000번째 While문.\n",
      "565000번째 While문.\n",
      "570000번째 While문.\n",
      "575000번째 While문.\n",
      "580000번째 While문.\n",
      "585000번째 While문.\n",
      "590000번째 While문.\n",
      "595000번째 While문.\n",
      "600000번째 While문.\n",
      "605000번째 While문.\n",
      "610000번째 While문.\n",
      "615000번째 While문.\n",
      "620000번째 While문.\n",
      "625000번째 While문.\n",
      "630000번째 While문.\n",
      "635000번째 While문.\n",
      "640000번째 While문.\n",
      "645000번째 While문.\n",
      "650000번째 While문.\n",
      "655000번째 While문.\n",
      "660000번째 While문.\n",
      "665000번째 While문.\n",
      "670000번째 While문.\n",
      "675000번째 While문.\n",
      "680000번째 While문.\n",
      "685000번째 While문.\n",
      "690000번째 While문.\n",
      "695000번째 While문.\n",
      "700000번째 While문.\n",
      "705000번째 While문.\n",
      "710000번째 While문.\n",
      "715000번째 While문.\n",
      "720000번째 While문.\n",
      "725000번째 While문.\n",
      "730000번째 While문.\n",
      "735000번째 While문.\n",
      "740000번째 While문.\n",
      "745000번째 While문.\n",
      "750000번째 While문.\n",
      "755000번째 While문.\n",
      "760000번째 While문.\n",
      "765000번째 While문.\n",
      "770000번째 While문.\n",
      "775000번째 While문.\n",
      "780000번째 While문.\n",
      "785000번째 While문.\n",
      "790000번째 While문.\n",
      "795000번째 While문.\n",
      "800000번째 While문.\n",
      "805000번째 While문.\n",
      "810000번째 While문.\n",
      "815000번째 While문.\n",
      "820000번째 While문.\n",
      "825000번째 While문.\n",
      "830000번째 While문.\n",
      "835000번째 While문.\n",
      "840000번째 While문.\n",
      "845000번째 While문.\n",
      "850000번째 While문.\n",
      "855000번째 While문.\n",
      "860000번째 While문.\n",
      "865000번째 While문.\n",
      "870000번째 While문.\n",
      "875000번째 While문.\n",
      "880000번째 While문.\n",
      "885000번째 While문.\n",
      "890000번째 While문.\n",
      "895000번째 While문.\n",
      "900000번째 While문.\n",
      "905000번째 While문.\n",
      "910000번째 While문.\n",
      "915000번째 While문.\n",
      "920000번째 While문.\n",
      "925000번째 While문.\n",
      "930000번째 While문.\n",
      "935000번째 While문.\n",
      "940000번째 While문.\n",
      "945000번째 While문.\n",
      "950000번째 While문.\n",
      "955000번째 While문.\n",
      "960000번째 While문.\n",
      "965000번째 While문.\n",
      "970000번째 While문.\n",
      "975000번째 While문.\n",
      "980000번째 While문.\n",
      "985000번째 While문.\n",
      "990000번째 While문.\n",
      "995000번째 While문.\n",
      "1000000번째 While문.\n",
      "1005000번째 While문.\n",
      "1010000번째 While문.\n",
      "1015000번째 While문.\n",
      "1020000번째 While문.\n",
      "1025000번째 While문.\n",
      "1030000번째 While문.\n",
      "1035000번째 While문.\n",
      "1040000번째 While문.\n",
      "1045000번째 While문.\n",
      "1050000번째 While문.\n",
      "1055000번째 While문.\n",
      "1060000번째 While문.\n",
      "1065000번째 While문.\n",
      "1070000번째 While문.\n",
      "1075000번째 While문.\n",
      "1080000번째 While문.\n",
      "1085000번째 While문.\n",
      "1090000번째 While문.\n",
      "1095000번째 While문.\n",
      "1100000번째 While문.\n",
      "1105000번째 While문.\n",
      "1110000번째 While문.\n",
      "1115000번째 While문.\n",
      "1120000번째 While문.\n",
      "1125000번째 While문.\n",
      "1130000번째 While문.\n",
      "1135000번째 While문.\n",
      "1140000번째 While문.\n",
      "1145000번째 While문.\n",
      "1150000번째 While문.\n",
      "1155000번째 While문.\n",
      "1160000번째 While문.\n",
      "1165000번째 While문.\n",
      "1170000번째 While문.\n",
      "1175000번째 While문.\n",
      "1180000번째 While문.\n",
      "1185000번째 While문.\n",
      "1190000번째 While문.\n",
      "1195000번째 While문.\n",
      "1200000번째 While문.\n",
      "1205000번째 While문.\n",
      "1210000번째 While문.\n",
      "1215000번째 While문.\n",
      "1220000번째 While문.\n",
      "1225000번째 While문.\n",
      "1230000번째 While문.\n",
      "1235000번째 While문.\n",
      "1240000번째 While문.\n",
      "1245000번째 While문.\n",
      "1250000번째 While문.\n",
      "1255000번째 While문.\n",
      "1260000번째 While문.\n",
      "1265000번째 While문.\n",
      "1270000번째 While문.\n",
      "1275000번째 While문.\n",
      "1280000번째 While문.\n",
      "1285000번째 While문.\n",
      "1290000번째 While문.\n",
      "1295000번째 While문.\n",
      "1300000번째 While문.\n",
      "1305000번째 While문.\n",
      "1310000번째 While문.\n",
      "1315000번째 While문.\n",
      "1320000번째 While문.\n",
      "1325000번째 While문.\n",
      "1330000번째 While문.\n",
      "1335000번째 While문.\n",
      "1340000번째 While문.\n",
      "1345000번째 While문.\n",
      "1350000번째 While문.\n",
      "1355000번째 While문.\n",
      "1360000번째 While문.\n",
      "1365000번째 While문.\n",
      "1370000번째 While문.\n",
      "1375000번째 While문.\n",
      "1380000번째 While문.\n",
      "1385000번째 While문.\n",
      "1390000번째 While문.\n",
      "1395000번째 While문.\n",
      "1400000번째 While문.\n",
      "1405000번째 While문.\n",
      "1410000번째 While문.\n",
      "1415000번째 While문.\n",
      "1420000번째 While문.\n",
      "1425000번째 While문.\n",
      "1430000번째 While문.\n",
      "1435000번째 While문.\n",
      "1440000번째 While문.\n",
      "1445000번째 While문.\n",
      "1450000번째 While문.\n",
      "1455000번째 While문.\n",
      "1460000번째 While문.\n",
      "1465000번째 While문.\n",
      "1470000번째 While문.\n",
      "1475000번째 While문.\n",
      "1480000번째 While문.\n",
      "1485000번째 While문.\n",
      "1490000번째 While문.\n",
      "1495000번째 While문.\n",
      "1500000번째 While문.\n",
      "1505000번째 While문.\n",
      "1510000번째 While문.\n",
      "1515000번째 While문.\n",
      "1520000번째 While문.\n",
      "1525000번째 While문.\n",
      "1530000번째 While문.\n",
      "1535000번째 While문.\n",
      "1540000번째 While문.\n",
      "1545000번째 While문.\n",
      "1550000번째 While문.\n",
      "1555000번째 While문.\n",
      "1560000번째 While문.\n",
      "1565000번째 While문.\n",
      "1570000번째 While문.\n",
      "1575000번째 While문.\n",
      "1580000번째 While문.\n",
      "1585000번째 While문.\n",
      "1590000번째 While문.\n",
      "1595000번째 While문.\n",
      "1600000번째 While문.\n",
      "1605000번째 While문.\n",
      "1610000번째 While문.\n",
      "1615000번째 While문.\n",
      "1620000번째 While문.\n",
      "1625000번째 While문.\n",
      "1630000번째 While문.\n",
      "1635000번째 While문.\n",
      "1640000번째 While문.\n",
      "1645000번째 While문.\n",
      "1650000번째 While문.\n",
      "1655000번째 While문.\n",
      "1660000번째 While문.\n",
      "1665000번째 While문.\n",
      "1670000번째 While문.\n",
      "1675000번째 While문.\n",
      "1680000번째 While문.\n",
      "1685000번째 While문.\n",
      "1690000번째 While문.\n",
      "1695000번째 While문.\n",
      "1700000번째 While문.\n",
      "1705000번째 While문.\n",
      "1710000번째 While문.\n",
      "1715000번째 While문.\n",
      "1720000번째 While문.\n",
      "1725000번째 While문.\n",
      "1730000번째 While문.\n",
      "1735000번째 While문.\n",
      "1740000번째 While문.\n",
      "1745000번째 While문.\n",
      "1750000번째 While문.\n",
      "1755000번째 While문.\n",
      "1760000번째 While문.\n",
      "1765000번째 While문.\n",
      "1770000번째 While문.\n",
      "1775000번째 While문.\n",
      "1780000번째 While문.\n",
      "1785000번째 While문.\n",
      "1790000번째 While문.\n",
      "1795000번째 While문.\n",
      "1800000번째 While문.\n",
      "1805000번째 While문.\n",
      "1810000번째 While문.\n",
      "1815000번째 While문.\n",
      "1820000번째 While문.\n",
      "1825000번째 While문.\n",
      "1830000번째 While문.\n",
      "1835000번째 While문.\n",
      "1840000번째 While문.\n",
      "1845000번째 While문.\n",
      "1850000번째 While문.\n",
      "1855000번째 While문.\n",
      "1860000번째 While문.\n",
      "1865000번째 While문.\n",
      "1870000번째 While문.\n",
      "1875000번째 While문.\n",
      "1880000번째 While문.\n",
      "1885000번째 While문.\n",
      "1890000번째 While문.\n",
      "1895000번째 While문.\n",
      "1900000번째 While문.\n",
      "1905000번째 While문.\n",
      "1910000번째 While문.\n",
      "1915000번째 While문.\n",
      "1920000번째 While문.\n",
      "1925000번째 While문.\n",
      "1930000번째 While문.\n",
      "1935000번째 While문.\n",
      "1940000번째 While문.\n",
      "1945000번째 While문.\n",
      "1950000번째 While문.\n",
      "1955000번째 While문.\n",
      "1960000번째 While문.\n",
      "1965000번째 While문.\n",
      "1970000번째 While문.\n",
      "1975000번째 While문.\n",
      "1980000번째 While문.\n",
      "1985000번째 While문.\n",
      "1990000번째 While문.\n",
      "1995000번째 While문.\n",
      "2000000번째 While문.\n",
      "2005000번째 While문.\n",
      "2010000번째 While문.\n",
      "2015000번째 While문.\n",
      "2020000번째 While문.\n",
      "2025000번째 While문.\n",
      "2030000번째 While문.\n",
      "2035000번째 While문.\n",
      "2040000번째 While문.\n",
      "2045000번째 While문.\n",
      "2050000번째 While문.\n",
      "2055000번째 While문.\n",
      "2060000번째 While문.\n",
      "2065000번째 While문.\n",
      "2070000번째 While문.\n",
      "2075000번째 While문.\n",
      "2080000번째 While문.\n",
      "2085000번째 While문.\n",
      "2090000번째 While문.\n",
      "2095000번째 While문.\n",
      "2100000번째 While문.\n",
      "2105000번째 While문.\n",
      "2110000번째 While문.\n",
      "2115000번째 While문.\n",
      "2120000번째 While문.\n",
      "2125000번째 While문.\n",
      "2130000번째 While문.\n",
      "2135000번째 While문.\n",
      "2140000번째 While문.\n",
      "2145000번째 While문.\n",
      "2150000번째 While문.\n",
      "2155000번째 While문.\n",
      "2160000번째 While문.\n",
      "2165000번째 While문.\n",
      "2170000번째 While문.\n",
      "2175000번째 While문.\n",
      "2180000번째 While문.\n",
      "2185000번째 While문.\n",
      "2190000번째 While문.\n",
      "2195000번째 While문.\n",
      "2200000번째 While문.\n",
      "2205000번째 While문.\n",
      "2210000번째 While문.\n",
      "2215000번째 While문.\n",
      "2220000번째 While문.\n",
      "2225000번째 While문.\n",
      "2230000번째 While문.\n",
      "2235000번째 While문.\n",
      "2240000번째 While문.\n",
      "2245000번째 While문.\n",
      "2250000번째 While문.\n",
      "2255000번째 While문.\n",
      "2260000번째 While문.\n",
      "2265000번째 While문.\n",
      "2270000번째 While문.\n",
      "2275000번째 While문.\n",
      "2280000번째 While문.\n",
      "2285000번째 While문.\n",
      "2290000번째 While문.\n",
      "2295000번째 While문.\n",
      "2300000번째 While문.\n",
      "2305000번째 While문.\n",
      "2310000번째 While문.\n",
      "2315000번째 While문.\n",
      "2320000번째 While문.\n",
      "2325000번째 While문.\n",
      "2330000번째 While문.\n",
      "2335000번째 While문.\n",
      "2340000번째 While문.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345000번째 While문.\n",
      "2350000번째 While문.\n",
      "2355000번째 While문.\n",
      "2360000번째 While문.\n",
      "2365000번째 While문.\n",
      "2370000번째 While문.\n",
      "2375000번째 While문.\n",
      "2380000번째 While문.\n",
      "2385000번째 While문.\n",
      "2390000번째 While문.\n",
      "2395000번째 While문.\n",
      "2400000번째 While문.\n",
      "2405000번째 While문.\n",
      "2410000번째 While문.\n",
      "2415000번째 While문.\n",
      "2420000번째 While문.\n",
      "2425000번째 While문.\n",
      "2430000번째 While문.\n",
      "2435000번째 While문.\n",
      "2440000번째 While문.\n",
      "2445000번째 While문.\n",
      "2450000번째 While문.\n",
      "2455000번째 While문.\n",
      "2460000번째 While문.\n",
      "2465000번째 While문.\n",
      "2470000번째 While문.\n",
      "2475000번째 While문.\n",
      "2480000번째 While문.\n",
      "2485000번째 While문.\n",
      "2490000번째 While문.\n",
      "2495000번째 While문.\n",
      "2500000번째 While문.\n",
      "2505000번째 While문.\n",
      "2510000번째 While문.\n",
      "2515000번째 While문.\n",
      "2520000번째 While문.\n",
      "2525000번째 While문.\n",
      "2530000번째 While문.\n",
      "2535000번째 While문.\n",
      "2540000번째 While문.\n",
      "2545000번째 While문.\n",
      "2550000번째 While문.\n",
      "2555000번째 While문.\n",
      "2560000번째 While문.\n",
      "2565000번째 While문.\n",
      "2570000번째 While문.\n",
      "2575000번째 While문.\n",
      "2580000번째 While문.\n",
      "2585000번째 While문.\n",
      "2590000번째 While문.\n",
      "2595000번째 While문.\n",
      "2600000번째 While문.\n",
      "2605000번째 While문.\n",
      "2610000번째 While문.\n",
      "2615000번째 While문.\n",
      "2620000번째 While문.\n",
      "2625000번째 While문.\n",
      "2630000번째 While문.\n",
      "2635000번째 While문.\n",
      "2640000번째 While문.\n",
      "2645000번째 While문.\n",
      "2650000번째 While문.\n",
      "2655000번째 While문.\n",
      "2660000번째 While문.\n",
      "2665000번째 While문.\n",
      "2670000번째 While문.\n",
      "2675000번째 While문.\n",
      "2680000번째 While문.\n",
      "2685000번째 While문.\n",
      "2690000번째 While문.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6e9595d7ada1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d번째 While문.\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtokenlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 단어 토큰화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "okt=Okt()\n",
    "fread = open('wiki_data.txt', encoding=\"utf8\")\n",
    "# 파일을 다시 처음부터 읽음.\n",
    "n=0\n",
    "result = []\n",
    "\n",
    "while True:\n",
    "    line = fread.readline() #한 줄씩 읽음.\n",
    "    if not line: break # 모두 읽으면 while문 종료.\n",
    "    n=n+1\n",
    "    if n%5000==0: # 5,000의 배수로 While문이 실행될 때마다 몇 번째 While문 실행인지 출력.\n",
    "        print(\"%d번째 While문.\"%n)\n",
    "    tokenlist = okt.pos(line, stem=True, norm=True) # 단어 토큰화\n",
    "    temp=[]\n",
    "    for word in tokenlist:\n",
    "        if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "            temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "      result.append(temp) # 결과에 저장\n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 형태소 분석기로 KoNLPy의 Okt를 사용하여 명사만을 추출하여 훈련 데이터를 구성하겠습니다. 위 작업은 시간이 꽤 걸립니다. 훈련 데이터를 모두 만들었다면, 훈련 데이터의 길이를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-791a3cf29425>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-791a3cf29425>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print('총 샘플의 개수 : {}'.format(len(result))\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 240만여개의 줄(line)이 명사 토큰화가 되어 저장되어 있는 상태입니다. 이제 이를 Word2Vec으로 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 다했다면 이제 임의의 입력 단어로부터 유사한 단어들을 구해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한국', 0.6791932582855225), ('조선민주주의인민공화국', 0.5612313747406006), ('우리나라', 0.5568702220916748), ('부산광역시', 0.5173649191856384), ('관세청', 0.49443531036376953), ('재일동포', 0.49121564626693726), ('대구광역시', 0.48996126651763916), ('지식경제부', 0.4863876700401306), ('방위사업청', 0.48310595750808716), ('대전광역시', 0.4814496636390686)]\n"
     ]
    }
   ],
   "source": [
    "model_result1 = model.wv.most_similar(\"대한민국\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('김지운', 0.8606835007667542), ('왕가위', 0.8571169376373291), ('허진호', 0.8384286165237427), ('김대승', 0.8354521989822388), ('임상수', 0.8257595300674438), ('이정범', 0.8235232830047607), ('봉준호', 0.822992205619812), ('오우삼', 0.8184781074523926), ('이장호', 0.8165385127067566), ('최동훈', 0.8163712024688721)]\n"
     ]
    }
   ],
   "source": [
    "model_result2 = model.wv.most_similar(\"홍상수\")\n",
    "print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('집적회로', 0.8066496253013611), ('실리콘', 0.8011645078659058), ('전자부품', 0.7966008186340332), ('태양전지', 0.7711134552955627), ('팹리스', 0.7657057642936707), ('페어차일드', 0.7538235187530518), ('웨이퍼', 0.7524611949920654), ('트랜지스터', 0.7400067448616028), ('전자제품', 0.7391147613525391), ('박막', 0.7331544160842896)]\n"
     ]
    }
   ],
   "source": [
    "model_result3 = model.wv.most_similar(\"반도체\")\n",
    "print(model_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다.\n",
    "\n",
    "예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능을 얻을 수 있습니다.\n",
    "\n",
    "여기서는 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습을 해보겠습니다. 실제로 모델에 적용해보는 실습은 사전 훈련된 워드 임베딩 챕터에서 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 영어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 구글이 제공하는 사전 훈련된(미리 학습되어져 있는) Word2Vec 모델을 사용하는 방법에 대해서 알아보도록 하겠습니다. 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공합니다. 각 임베딩 벡터의 차원은 300입니다. gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다.\n",
    "\n",
    "모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "압축 파일의 용량은 약 1.5GB이지만, 파일의 압축을 풀면 약 3.3GB의 파일이 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드합니다.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40797037\n",
      "0.057204384\n"
     ]
    }
   ],
   "source": [
    "print (model.similarity('this', 'is')) # 두 단어의 유사도 계산하기\n",
    "print (model.similarity('post', 'book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11279297 -0.02612305 -0.04492188  0.06982422  0.140625    0.03039551\n",
      " -0.04370117  0.24511719  0.08740234 -0.05053711  0.23144531 -0.07470703\n",
      "  0.21875     0.03466797 -0.14550781  0.05761719  0.00671387 -0.00701904\n",
      "  0.13183594 -0.25390625  0.14355469 -0.140625   -0.03564453 -0.21289062\n",
      " -0.24804688  0.04980469 -0.09082031  0.14453125  0.05712891 -0.10400391\n",
      " -0.19628906 -0.20507812 -0.27539062  0.03063965  0.20117188  0.17382812\n",
      "  0.09130859 -0.10107422  0.22851562 -0.04077148  0.02709961 -0.00106049\n",
      "  0.02709961  0.34179688 -0.13183594 -0.078125    0.02197266 -0.18847656\n",
      " -0.17480469 -0.05566406 -0.20898438  0.04858398 -0.07617188 -0.15625\n",
      " -0.05419922  0.01672363 -0.02722168 -0.11132812 -0.03588867 -0.18359375\n",
      "  0.28710938  0.01757812  0.02185059 -0.05664062 -0.01251221  0.01708984\n",
      " -0.21777344 -0.06787109  0.04711914 -0.00668335  0.08544922 -0.02209473\n",
      "  0.31835938  0.01794434 -0.02246094 -0.03051758 -0.09570312  0.24414062\n",
      "  0.20507812  0.05419922  0.29101562  0.03637695  0.04956055 -0.06689453\n",
      "  0.09277344 -0.10595703 -0.04370117  0.19726562 -0.03015137  0.05615234\n",
      "  0.08544922 -0.09863281 -0.02392578 -0.08691406 -0.22460938 -0.16894531\n",
      "  0.09521484 -0.0612793  -0.03015137 -0.265625   -0.13378906  0.00139618\n",
      "  0.01794434  0.10107422  0.13964844  0.06445312 -0.09765625 -0.11376953\n",
      " -0.24511719 -0.15722656  0.00457764  0.12988281 -0.03540039 -0.08105469\n",
      "  0.18652344  0.03125    -0.09326172 -0.04760742  0.23730469  0.11083984\n",
      "  0.08691406  0.01916504  0.21386719 -0.0065918  -0.08984375 -0.02502441\n",
      " -0.09863281 -0.05639648 -0.26757812  0.19335938 -0.08886719 -0.25976562\n",
      "  0.05957031 -0.10742188  0.09863281  0.1484375   0.04101562  0.00340271\n",
      " -0.06591797 -0.02941895  0.20019531 -0.00521851  0.02355957 -0.13671875\n",
      " -0.12597656 -0.10791016  0.0067749   0.15917969  0.0145874  -0.15136719\n",
      "  0.07519531 -0.02905273  0.01843262  0.20800781  0.25195312 -0.11523438\n",
      " -0.23535156  0.04101562 -0.11035156  0.02905273  0.22460938 -0.04272461\n",
      "  0.09667969  0.11865234  0.08007812  0.07958984  0.3125     -0.14941406\n",
      " -0.234375    0.06079102  0.06982422 -0.14355469 -0.05834961 -0.36914062\n",
      " -0.10595703  0.00738525  0.24023438 -0.10400391 -0.02124023  0.05712891\n",
      " -0.11621094 -0.16894531 -0.06396484 -0.12060547  0.08105469 -0.13769531\n",
      " -0.08447266  0.12792969 -0.15429688  0.17871094  0.2421875  -0.06884766\n",
      "  0.03320312  0.04394531 -0.04589844  0.03686523 -0.07421875 -0.01635742\n",
      " -0.24121094 -0.08203125 -0.01733398  0.0291748   0.10742188  0.11279297\n",
      "  0.12890625  0.01416016 -0.28710938  0.16503906 -0.25585938  0.2109375\n",
      " -0.19238281  0.22363281  0.04541016  0.00872803  0.11376953  0.375\n",
      "  0.09765625  0.06201172  0.12109375 -0.24316406  0.203125    0.12158203\n",
      "  0.08642578  0.01782227  0.17382812  0.01855469  0.03613281 -0.02124023\n",
      " -0.02905273 -0.04541016  0.1796875   0.06494141 -0.13378906 -0.09228516\n",
      "  0.02172852  0.02099609  0.07226562  0.3046875  -0.27539062 -0.30078125\n",
      "  0.08691406 -0.22949219  0.0546875  -0.34179688 -0.00680542 -0.0291748\n",
      " -0.03222656  0.16210938  0.01141357  0.23339844 -0.0859375  -0.06494141\n",
      "  0.15039062  0.17675781  0.08251953 -0.26757812 -0.11669922  0.01330566\n",
      "  0.01818848  0.10009766 -0.09570312  0.109375   -0.16992188 -0.23046875\n",
      " -0.22070312  0.0625      0.03662109 -0.125       0.05151367 -0.18847656\n",
      "  0.22949219  0.26367188 -0.09814453  0.06176758  0.11669922  0.23046875\n",
      "  0.32617188  0.02038574 -0.03735352 -0.12255859  0.296875   -0.25\n",
      " -0.08544922 -0.03149414  0.38085938  0.02929688 -0.265625    0.42382812\n",
      " -0.1484375   0.14355469 -0.03125     0.00717163 -0.16601562 -0.15820312\n",
      "  0.03637695 -0.16796875 -0.01483154  0.09667969 -0.05761719 -0.00515747]\n"
     ]
    }
   ],
   "source": [
    "print(model['book']) # 단어 'book'의 벡터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 한국어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어의 미리 학습된 Word2Vec 모델은 박규병님의 깃허브 주소인 https://github.com/Kyubyong/wordvectors 에 공개되어져 있습니다. 박규병님이 공개한 직접적인 다운로드 링크는 아래와 같습니다.\n",
    "\n",
    "모델 다운로드 경로 : https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view\n",
    "\n",
    "위의 링크로부터 77MB 크기의 ko.zip 파일을 다운로드 받아서 압축을 풀면 ko.bin이라는 50MB 크기의 파일이 있습니다. 이 파일을 로드하고 유사도를 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('고양이', 0.7290452718734741), ('거위', 0.7185635566711426), ('토끼', 0.7056223154067993), ('멧돼지', 0.6950401067733765), ('엄마', 0.6934334635734558), ('난쟁이', 0.6806551218032837), ('한마리', 0.6770296096801758), ('아가씨', 0.6750352382659912), ('아빠', 0.6729634404182434), ('목걸이', 0.6512460708618164)]\n"
     ]
    }
   ],
   "source": [
    "result = model.wv.most_similar(\"강아지\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : Word2vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이지만 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델입니다. 우선 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Word2Vec 구현하기(Skip-Gram with Negative Sampling, SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 네거티브 샘플링을 사용한 Skip-Gram(Skip-Gram with Negative Sampling, SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 20뉴스그룹 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA 챕터에서 사용했던 20뉴스그룹 데이터를 재사용합니다. 이번 실습에서는 하나의 샘플에 최소 단어 2개는 있어야 합니다. 그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생합니다. 전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 샘플 수는 11,314개입니다. 전처리를 진행해봅시다. 불필요한 토큰을 제거하고, 소문자화를 통해 정규화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 데이터프레임에 Null 값이 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null 값이 없지만, 빈 값(empy) 유무도 확인해야 합니다. 모든 빈 값을 Null 값으로 변환하고, 다시 Null 값이 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 :',len(news_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 수가 일부 줄어든 것을 확인할 수 있습니다. NLTK에서 정의한 불용어 리스트를 사용하여 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어를 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거하였으므로 단어의 수가 줄어들었습니다. 모든 샘플 중 단어가 1개 이하인 경우를 모두 찾아 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 10940\n"
     ]
    }
   ],
   "source": [
    "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print('총 샘플 수 :',len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 수가 다시 줄어들었습니다. 단어 집합을 생성하고, 정수 인코딩을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 2개의 샘플을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 집합의 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1 \n",
    "print('단어 집합의 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 네거티브 샘플링을 통한 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정을 거쳤습니다. 네거티브 샘플링을 통한 데이터셋을 구성할 차례입니다. 이를 위해서는 네거티브 샘플링을 위해서 케라스에서 제공하는 전처리 도구인 skipgrams를 사용합니다. 어떤 전처리가 수행되는지 그 결과를 확인하기 위해서 (꽤 시간이 소요되는 작업이므로) 상위 10개의 뉴스그룹 샘플에 대해서만 수행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "# 네거티브 샘플링\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 확인합니다. 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(holocaust (2669), makes (228)) -> 1\n",
      "(existance (4865), initialize (8803)) -> 0\n",
      "(holocaust (2669), tracable (53916)) -> 0\n",
      "(media (702), tenor (15891)) -> 0\n",
      "(daily (1920), might (52)) -> 1\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          idx2word[pairs[i][0]], pairs[i][0], \n",
    "          idx2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성합니다. 이 과정은 각각의 뉴스그룹 샘플에 대해서 동일한 프로세스로 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다. 그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있습니다. 첫번째 뉴스그룹 샘플이 가지고 있는 pairs와 labels의 개수를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 작업을 모든 뉴스그룹 샘플에 대해서 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-Gram with Negative Sampling(SGNS) 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram을 직접 구현해봅시다. 우선 필요한 도구들을 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 100으로 정했습니다. 이는 사용자가 정하는 하이퍼파라미터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 설계해보겠습니다. 우선, 두 개의 임베딩 테이블을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding  = Embedding(vocab_size, embed_size)(c_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며 각 단어는 임베딩 테이블을 거쳐서 내적을 수행하고, 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\jikim\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\jikim\\appdata\\roaming\\python\\python37\\site-packages (from pydot) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\jikim\\anaconda3\\lib\\site-packages (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 100)       6427700     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 100)       6427700     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 1)         0           embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1)            0           dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지면의 한계로 인해 summary() 결과는 생략하였으나 총 파라미터 수는 12,855,400입니다. 모델의 학습은 5에포크 수행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 10 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 47 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 30 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 30 calls to <function Model.make_train_function.<locals>.train_function at 0x000002522F3F2318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-0b78c319fe70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfirst_elem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_elem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch :'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Loss :'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                                     class_weight)\n\u001b[0;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "학습 시간은 꽤 걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 임베딩 벡터들을 vector.txt에 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 gensim의 .models.KeyedVectors.load_word2vec_format로 로드하면 쉽게 단어 간 유사도를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('charles', 0.9281303882598877),\n",
       " ('planet', 0.9280704855918884),\n",
       " ('regime', 0.9275240898132324),\n",
       " ('cease', 0.926586925983429),\n",
       " ('abilities', 0.9258583784103394),\n",
       " ('provision', 0.9252725839614868),\n",
       " ('restrictions', 0.925165057182312),\n",
       " ('workstations', 0.9250816106796265),\n",
       " ('popped', 0.9244874715805054),\n",
       " ('monitoring', 0.9237146973609924)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bait', 0.9512479901313782),\n",
       " ('advertised', 0.9503906965255737),\n",
       " ('pixmap', 0.9499825239181519),\n",
       " ('agencies', 0.948897123336792),\n",
       " ('assured', 0.9487747550010681),\n",
       " ('indicated', 0.9486934542655945),\n",
       " ('importantly', 0.947441816329956),\n",
       " ('variations', 0.9473994970321655),\n",
       " ('inferior', 0.9471836090087891),\n",
       " ('watt', 0.9469276666641235)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disarming', 0.9452154040336609),\n",
       " ('gotta', 0.9430790543556213),\n",
       " ('cryptosystem', 0.9430403709411621),\n",
       " ('racing', 0.9423022270202637),\n",
       " ('promiscuous', 0.9423009157180786),\n",
       " ('possession', 0.9421066045761108),\n",
       " ('potvin', 0.9419005513191223),\n",
       " ('ethics', 0.9416452646255493),\n",
       " ('commerce', 0.9405856728553772),\n",
       " ('directed', 0.9404736161231995)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('advisory', 0.9483902454376221),\n",
       " ('potvin', 0.9453271627426147),\n",
       " ('shame', 0.9448542594909668),\n",
       " ('essensa', 0.9438319206237793),\n",
       " ('ability', 0.9433873295783997),\n",
       " ('imprisonment', 0.9432370662689209),\n",
       " ('continental', 0.9423372745513916),\n",
       " ('mice', 0.9417917728424072),\n",
       " ('irrelevant', 0.9414211511611938),\n",
       " ('newer', 0.9411406517028809)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cellular', 0.8919676542282104),\n",
       " ('translations', 0.8888408541679382),\n",
       " ('usage', 0.8863200545310974),\n",
       " ('unless', 0.8862480521202087),\n",
       " ('pointing', 0.8858127593994141),\n",
       " ('digital', 0.8856213092803955),\n",
       " ('flesh', 0.8854401707649231),\n",
       " ('hint', 0.8851814270019531),\n",
       " ('anthony', 0.8850693702697754),\n",
       " ('synchronous', 0.8838422298431396)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
